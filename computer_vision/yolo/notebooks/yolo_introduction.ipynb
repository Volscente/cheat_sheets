{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccaca3a7-c8d0-4dba-9219-011fd83e323b",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Implement YOLO usage as explained in the following articles:\n",
    "[reference article](https://towardsdatascience.com/yolo-object-detection-with-opencv-and-python-21e50ac599e9).\n",
    "\n",
    "<br>\n",
    "\n",
    "The model has been trained over the [COCO Dataset](https://cocodataset.org/#home)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01978881-349a-4132-8aa2-afa4913d80a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Standard Libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "911b345d-0771-4483-a70f-9b6ae0d8f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook variables\n",
    "sample_image_path = './../images/dog_image_1.jpeg'\n",
    "classes_path = './../files/yolov3_classes.txt'\n",
    "nn_config_path = './../files/yolov3.cfg'\n",
    "nn_weights_url = 'https://pjreddie.com/media/files/yolov3.weights'\n",
    "nn_weights_path = './../files/yolov3.weights'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f510b96d-2c51-4b08-a856-4da6a440d948",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11754f8c-cafe-422b-9d86-c6aef3781129",
   "metadata": {},
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "621458eb-7e0b-4470-ab8d-79598a063d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image with OpenCV\n",
    "image = cv2.imread(sample_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "380a6142-a2a7-4040-b8ca-025870408bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 768, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The image is 576 x 768 dimension in pixels\n",
    "# It has 3 channels, since it is colored (RGB)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1613c106-1df4-419a-b6c9-5566bf8116e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute image's width and height\n",
    "image_width = image.shape[1]\n",
    "image_height = image.shape[0]\n",
    "\n",
    "# Define the scale factor for each image's pixel\n",
    "scale = 0.00392"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73f6b1-a7df-4199-97b4-a04895701d41",
   "metadata": {},
   "source": [
    " ## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f99a8726-a792-48a3-8a59-c4ce3660a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the classes file and extract the list of available classes\n",
    "with open(classes_path, 'r') as classes_file:\n",
    "    classes = [line.strip() for line in classes_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc679149-6448-48c1-8d02-29e3231683e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate different colors for each class \n",
    "class_colors = np.random.uniform(0, 255, size=(len(classes), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e29cfdc-61da-4853-bd14-b2c780c6ad8e",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "The neural network is divided into two files:\n",
    "1. **yolov3.weights** - It contains the weights of the single neuron in the Neural Network\n",
    "2. **yolov3.cfg** - It contains the structure of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0baa1038-19c2-4b81-9917-96ed483442cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whatever the 'yolov3.weights' file is not present and download it\n",
    "if not os.path.isfile(nn_weights_path):\n",
    "    \n",
    "    # Download 'yolov3.weights'\n",
    "    urlretrieve(nn_weights_url, nn_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca8b0ff-ffc9-4828-8ead-d6f6cd7c92aa",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945838ca-d5d8-4938-a864-010b4b6072f5",
   "metadata": {},
   "source": [
    "## Create the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b0c5f64-775e-44e3-aa00-71f66fc31f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pr-trained model and configuration file if the required files are available\n",
    "if os.path.isfile(nn_weights_path) and os.path.isfile(nn_config_path):\n",
    "    \n",
    "    neural_network = cv2.dnn.readNetFromDarknet(nn_config_path, nn_weights_path)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('Missing required files: yolov3.weights and yolov3.cfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3b3a73-3cfc-4472-8e69-31b3fd537a9d",
   "metadata": {},
   "source": [
    "## Create Blob Object from Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77b61ddc-fb38-4db4-87cd-a9c40dcea670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 4-dimensional Blob from an image\n",
    "blob_image = cv2.dnn.blobFromImage(image=image, # Input image for the NN\n",
    "                                   size=(416,416), # Resize the image\n",
    "                                   mean=(0, 0, 0), # Do not multiply channels for their mean, set to zero\n",
    "                                   scalefactor=scale, # Scale each pixel value (swapRB switches the first and last channel)\n",
    "                                   swapRB=True, # Swap first and last channel\n",
    "                                   crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a7ba5a3-f7d8-46f9-a829-62250a0039d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Blob as Neural Network's input\n",
    "neural_network.setInput(blob_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168319ba-2e75-456a-bebc-ec9738dfb6cc",
   "metadata": {},
   "source": [
    "# Utils Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a14fdc-c180-4e20-9ea9-dda6025b6b62",
   "metadata": {},
   "source": [
    "## Draw Bounding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90d56604-21ef-444c-8f71-1d63eb20e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_bow(image: np.ndarray, \n",
    "                      class_id: int, \n",
    "                      point_1: Tuple[float, float], \n",
    "                      point_2: Tuple[float, float]) -> None:\n",
    "    \"\"\"\n",
    "    Draw a bounding box over the passed image from the points 1 and 2\n",
    "    \n",
    "    Parameters:\n",
    "        image: numpy.ndarray of image shape (n, m, 3)\n",
    "        class_id: Integer of class color\n",
    "        point_1: Tuple of floats for x and y coordinates\n",
    "        point_2: Tuple of floats for x and y coordinates\n",
    "    \n",
    "    Returns:\n",
    "        Draw bounding box over the image\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the label\n",
    "    label = classes[class_id]\n",
    "    \n",
    "    # Retrieve the color\n",
    "    color = class_colors[class_id]\n",
    "    \n",
    "    # Draw the bounding box\n",
    "    cv2.rectangle(image, \n",
    "                  point_1, \n",
    "                  point_2, \n",
    "                  color, \n",
    "                  2)\n",
    "    \n",
    "    # Put the text over the bounding box\n",
    "    cv2.putText(image, \n",
    "                label, \n",
    "                (point_1[0] - 10, point_1[0] - 10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.5, \n",
    "                color, \n",
    "                2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b30c39d-a211-4c6b-8267-3c326c5bacb3",
   "metadata": {},
   "source": [
    "## Retrieve Output Layers\n",
    "\n",
    "YOLO v3 architecture has 3 output layers and it is required to retrieve their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91cce75e-05f7-4e0b-ac27-2513a55939c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_layers(neural_network: cv2.dnn.Net) -> List:\n",
    "    \"\"\"\n",
    "    Retrieve the list of output layers names\n",
    "    \n",
    "    Parameters:\n",
    "        neural_network: cv.dnn.Net neural network instance\n",
    "        \n",
    "    Returns:\n",
    "        output_layers: List of output layers names\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reitreve layer's names\n",
    "    layer_names = neural_network.getLayerNames()\n",
    "    \n",
    "    # Get output layers names since by the non-output connected ones\n",
    "    output_layers = [layer_names[i - 1] for i in neural_network.getUnconnectedOutLayers()]\n",
    "    \n",
    "    return output_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c875d1-0b2c-4b66-875e-223cc717aebc",
   "metadata": {},
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1531bf-cc49-4045-8ac0-4d817a548f36",
   "metadata": {},
   "source": [
    "## Feed Forward the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fe71449-dbaa-465a-a091-8fb9e9faf62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the model's output of only the output layers\n",
    "outputs = neural_network.forward(get_output_layers(neural_network))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e391621-e076-4ba6-bb74-fb826f5380c0",
   "metadata": {},
   "source": [
    "## Retrieve Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1dcad586-2f80-4693-a9a8-8b5898832858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the detected classes, confidences and boxes\n",
    "detected_classes, detected_confidences, detected_boxes = [], [], [] \n",
    "\n",
    "# Define confidence threshold\n",
    "confidence_threshold = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0a074c2-4b50-425e-a569-dd65578f8df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch every detection captured by the Neural Network\n",
    "for output in outputs:\n",
    "    \n",
    "    for detection in output:\n",
    "    \n",
    "        # Retrieve the score of the detection for each class (First 4 values are the box coordinates)\n",
    "        scores = detection[:5]\n",
    "        \n",
    "        # Get the maximum score, which corresponds to the detected class\n",
    "        detected_class = np.argmax(scores)\n",
    "        \n",
    "        # Retrieve the detected confidence level\n",
    "        detected_confidence = scores[detected_class]\n",
    "        \n",
    "        # Check if the confidence is greater than the threshold\n",
    "        if detected_confidence > confidence_threshold:\n",
    "            \n",
    "            # Retrieve box coordinates\n",
    "            center_x = int(detection[0] * image_width)\n",
    "            center_y = int(detection[1] * image_height)\n",
    "            w = int(detection[2] * image_width)\n",
    "            h = int(detection[3] * image_height)\n",
    "            x = center_x - w / 2\n",
    "            y = center_y - h / 2\n",
    "            \n",
    "            # Update lists\n",
    "            detected_classes.append(detected_class)\n",
    "            detected_confidences.append(float(detected_confidence))\n",
    "            detected_boxes.append([x, y, w, h])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c2b16-eb01-4323-9af7-6390884d60c2",
   "metadata": {},
   "source": [
    "## Apply Non-Max Suppression to the Detected Boxes\n",
    "\n",
    "It is used to reduce the number of detected boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54d0aca3-4d84-4dcf-8d7b-eaf40872e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Non-Max Suppression Threshold\n",
    "nms_threshold = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec72bc8f-6a61-442e-8abd-47e623306b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Non-Max Suppression\n",
    "indices = cv2.dnn.NMSBoxes(detected_boxes, detected_confidences, confidence_threshold, nms_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b674de51-2528-416c-8ef9-0d854d985b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feth the indices of the boxes\n",
    "for index in indices:\n",
    "    \n",
    "    # Retrieve box coordinates\n",
    "    box = detected_boxes[index]\n",
    "    x = box[0]\n",
    "    y = box[1]\n",
    "    w = box[2]\n",
    "    h = box[3]\n",
    "    \n",
    "    # Draw the Bounding Box\n",
    "    draw_bounding_bow(image, detected_classes[index], (round(x), round(y)), (round(x+w), round(y+h)))\n",
    "    \n",
    "# Save output image to disk\n",
    "cv2.imwrite(\"object-detection.jpg\", image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
