{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4307a243-8d79-4bd9-aa60-4612097fc71e",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The notebook is intended to experiment with the Subclassing API of TensorFlow to define custom Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab7906f-188a-42b4-97f5-b463f0cfdd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Standard Libraries\n",
    "import os\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312864af-a7fa-41a8-8c46-8948de2a3448",
   "metadata": {},
   "source": [
    "# Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d9dd48b-1701-4768-995a-541f374c5445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Define a custom Linear Layer in Keras\n",
    "\n",
    "    Attributes:\n",
    "        weights_params: tf.Variables set of weights for each neuron\n",
    "        bias: tf.Variables set of biases for each neuron\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 units=32):\n",
    "\n",
    "        # Call the parent constructor             \n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        # Initilialize the weights\n",
    "        self.weights_params = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=tf.random_normal_initializer(),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        # Initilialize the bias             \n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        return tf.matmul(inputs, self.weights_params) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2da9b71f-bb22-4113-9e3b-39048da0a3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.01872915 -0.06430404 -0.0005488  -0.01024083]\n",
      " [-0.01872915 -0.06430404 -0.0005488  -0.01024083]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "x = tf.ones((2, 2))\n",
    "linear_layer = Linear(4)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6e9b0d4-033b-4a57-89f2-b9692f5ea03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# NOTE: The layer has already a \"weights\" attribute\n",
    "print(linear_layer.weights == [linear_layer.weights_params, linear_layer.bias])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71742a65-ca7f-4b56-9018-0361de4911a8",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron Block\n",
    "\n",
    "It creates a block layer that combines three custom Linear layers. It tracks the weights of the inner layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14e1c012-6a32-4723-a7c7-ed334315180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "         # Call the parent constructor\n",
    "        super(MLPBlock, self).__init__()\n",
    "\n",
    "        # Define hidden layers\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # Pass input to the first hidden layer\n",
    "        x = self.linear_1(inputs)\n",
    "\n",
    "        # Trigger the first hidden layer activation function\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        # Pass input to the second hidden layer\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        # Trigger the second hidden layer activation function\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        return self.linear_3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8f7bcbc-d3c9-4b7c-a82a-ae8ac2d583b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 6\n",
      "trainable weights: 6\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "mlp = MLPBlock()\n",
    "\n",
    "y = mlp(tf.ones(shape=(3, 64)))  # The first call to the `mlp` will create the weights\n",
    "\n",
    "print(\"weights:\", len(mlp.weights)) # 3 vector of weights_params and 3 vector of bias\n",
    "print(\"trainable weights:\", len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12deaeb-eb3b-41d7-9697-4787af68194b",
   "metadata": {},
   "source": [
    "# Layer Loss\n",
    "\n",
    "By adding the `add_loss()` function to a specific Layer it is possible to compute the Layer Loss between its inputs and outputs.\n",
    "\n",
    "We are going to create an Activity Regularization Layer that encouragse the neural network's activations or outputs to exhibit certain desirable properties. Activity regularization losses are additional terms added to the overall loss function during training to promote specific characteristics in the activations of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ceb1fa4-55eb-4743-a9b6-b1287d03304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityRegularizationLayer(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 dropout_rate=1e-2):\n",
    "\n",
    "        # Call the parent constructor\n",
    "        super(ActivityRegularizationLayer, self).__init__()\n",
    "                     \n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        self.add_loss(self.dropout_rate * tf.reduce_sum(inputs))\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e88bc9c-1e64-4aa8-8909-a464333804f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "class OuterLayer(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.activity_reg = ActivityRegularizationLayer(1e-2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.activity_reg(inputs)\n",
    "\n",
    "\n",
    "layer = OuterLayer()\n",
    "print(len(layer.losses) == 0)  # No losses yet since the layer has never been called\n",
    "\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "print(len(layer.losses) == 1)  # We created one loss value\n",
    "\n",
    "# `layer.losses` gets reset at the start of each __call__\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "print(len(layer.losses) == 1)  # This is the loss created during the call above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acdb53d9-49da-4192-8a11-1c3baad428fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.0016005059>]\n"
     ]
    }
   ],
   "source": [
    "# It's also possible to use Subclassing to add a Layer Loss in a Dense Layer\n",
    "class OuterLayerWithKernelRegularizer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OuterLayerWithKernelRegularizer, self).__init__()\n",
    "        \n",
    "        self.dense = keras.layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "\n",
    "layer = OuterLayerWithKernelRegularizer()\n",
    "_ = layer(tf.zeros((1, 1)))\n",
    "\n",
    "print(layer.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dd1b7c-dd2d-4045-be88-330c5fe9acd9",
   "metadata": {},
   "source": [
    "# Layer Metric\n",
    "\n",
    "It's the same logic applied to Layer Loss: track the metric for the specific layer during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e46a9dc6-764b-4163-83ec-df9b771f8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticEndpoint(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, name=None):\n",
    "\n",
    "        # Call the parent constructor\n",
    "        super(LogisticEndpoint, self).__init__(name=name)\n",
    "\n",
    "        # Define loss and metric functions\n",
    "        self.loss_function = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.metric_function = keras.metrics.BinaryAccuracy()\n",
    "\n",
    "    def call(self, targets, logits, sample_weights=None):\n",
    "\n",
    "        # Compute and add the layer loss\n",
    "        loss = self.loss_function(targets, logits, sample_weights)\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # Compute and add the layer metric\n",
    "        metric = self.metric_function(targets, logits, sample_weights)\n",
    "        self.add_metric(metric, name=\"accuracy\")\n",
    "\n",
    "        return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e65b018-1e37-44f2-b1d5-74ff3422c36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.metrics: [<keras.src.metrics.accuracy_metrics.BinaryAccuracy object at 0x1647c4e20>]\n",
      "current accuracy value: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "layer = LogisticEndpoint()\n",
    "\n",
    "targets = tf.ones((2, 2))\n",
    "logits = tf.ones((2, 2))\n",
    "y = layer(targets, logits)\n",
    "\n",
    "print(\"layer.metrics:\", layer.metrics)\n",
    "print(\"current accuracy value:\", float(layer.metrics[0].result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d2b35-58a5-43fc-bf8a-0860e1aed8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
