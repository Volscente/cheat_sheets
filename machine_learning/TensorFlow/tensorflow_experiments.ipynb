{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b86efb9-e5da-4904-9cda-739b08053443",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The notebook is intended to experiment with the different TensorFlow APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ce2cacb-a45c-471d-bc46-a28743cb89ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Standard Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cefb6a3-8d26-4616-8dcd-6c8eaae13af8",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d6df178-17bc-4a14-93dd-6981b16a17b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset\n",
    "X = tf.constant(range(10), dtype=tf.float32)\n",
    "Y = 2 * X + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20444ff-7e89-4f20-bf59-f63f50360545",
   "metadata": {},
   "source": [
    "# Dataset API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055c2518-54cb-49d9-8f48-62ccac04dac7",
   "metadata": {},
   "source": [
    "## Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0272685-6353-49eb-8214-d145e6801113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97c5aa7-a70f-40af-830c-e33be82c7ec6",
   "metadata": {},
   "source": [
    "## Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "625edc2f-8d0b-499c-9eef-c505d6fa9c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 - 10.0\n",
      "1.0 - 12.0\n",
      "2.0 - 14.0\n",
      "3.0 - 16.0\n",
      "4.0 - 18.0\n",
      "5.0 - 20.0\n",
      "6.0 - 22.0\n",
      "7.0 - 24.0\n",
      "8.0 - 26.0\n",
      "9.0 - 28.0\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the data samples\n",
    "for x, y in dataset:\n",
    "    print(f'{x} - {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c9ad2c-51c0-407d-bdb3-5a00dccb822b",
   "metadata": {},
   "source": [
    "## Batch and Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbf473c4-88ed-453e-a902-8cff11e4d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and epochs\n",
    "batch_size = 3\n",
    "epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c516f058-397e-4928-846c-8879753848ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add batches and epochs\n",
    "dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e352dab-b0f1-45c7-a2c9-302f23a9792e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2.] - [10. 12. 14.]\n",
      "[3. 4. 5.] - [16. 18. 20.]\n",
      "[6. 7. 8.] - [22. 24. 26.]\n",
      "[9. 0. 1.] - [28. 10. 12.]\n",
      "[2. 3. 4.] - [14. 16. 18.]\n",
      "[5. 6. 7.] - [20. 22. 24.]\n",
      "[8. 9. 0.] - [26. 28. 10.]\n",
      "[1. 2. 3.] - [12. 14. 16.]\n",
      "[4. 5. 6.] - [18. 20. 22.]\n",
      "[7. 8. 9.] - [24. 26. 28.]\n",
      "[0. 1. 2.] - [10. 12. 14.]\n",
      "[3. 4. 5.] - [16. 18. 20.]\n",
      "[6. 7. 8.] - [22. 24. 26.]\n",
      "[9. 0. 1.] - [28. 10. 12.]\n",
      "[2. 3. 4.] - [14. 16. 18.]\n",
      "[5. 6. 7.] - [20. 22. 24.]\n",
      "[8. 9. 0.] - [26. 28. 10.]\n",
      "[1. 2. 3.] - [12. 14. 16.]\n",
      "[4. 5. 6.] - [18. 20. 22.]\n",
      "[7. 8. 9.] - [24. 26. 28.]\n"
     ]
    }
   ],
   "source": [
    "# Reitreve the batches for the epochs\n",
    "for batch_x, batch_y in dataset:\n",
    "    print(f'{batch_x} - {batch_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b4f3c-d356-42d0-bd42-5f269645ce2e",
   "metadata": {},
   "source": [
    "# Linear Regression Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b478b477-37d9-462d-9602-2c244d467b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse(X, Y, w0, b):\n",
    "    \"\"\"\n",
    "    Compute the Loss as Mean Squared Error\n",
    "    \"\"\"\n",
    "    \n",
    "    # Predict the y value\n",
    "    y_predicted = w0 * X + b\n",
    "\n",
    "    # Compute the error\n",
    "    errors = (y_predicted - Y)**2\n",
    "\n",
    "    # Compute the mean\n",
    "    mse = tf.reduce_mean(errors)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dd26993-a375-41cb-805c-d4aebbbccc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_derivate_terms(X, Y, w0, b):\n",
    "    \"\"\"\n",
    "    Compute the derivative terms for w0 and b\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_mse(X, Y, w0, b)\n",
    "    return tape.gradient(loss, [w0, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98a8820c-e908-416e-9879-f93935ca77f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 21:28:26.118222: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [10]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 - Loss 121.37281799316406\n",
      "Step 100 - Loss 67.96109008789062\n",
      "Step 200 - Loss 60.568275451660156\n",
      "Step 300 - Loss 54.036888122558594\n",
      "Step 400 - Loss 48.209930419921875\n",
      "Step 500 - Loss 43.01130676269531\n",
      "Step 600 - Loss 38.37327575683594\n",
      "Step 700 - Loss 34.235374450683594\n",
      "Step 800 - Loss 30.54366683959961\n",
      "Step 900 - Loss 27.25004768371582\n",
      "Step 1000 - Loss 24.31158447265625\n",
      "Step 1100 - Loss 21.689990997314453\n",
      "Step 1200 - Loss 19.35108184814453\n"
     ]
    }
   ],
   "source": [
    "# Initialise w0 and b\n",
    "w0, b = tf.Variable(0.0), tf.Variable(0.0)\n",
    "\n",
    "# Set learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create bigger dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, Y)).repeat(250).batch(2, drop_remainder=True)\n",
    "\n",
    "for step, (X_batch, Y_batch) in enumerate(dataset):\n",
    "\n",
    "    # Compute the derivate terms\n",
    "    dw0, db = compute_derivate_terms(X_batch, Y_batch, w0, b)\n",
    "\n",
    "    # Update w0 and b\n",
    "    w0.assign_sub(dw0 * learning_rate)\n",
    "    b.assign_sub(db * learning_rate)\n",
    "\n",
    "    # Print loss every 100 steps\n",
    "    if step % 100 == 0:\n",
    "        loss = loss_mse(X_batch, Y_batch, w0, b)\n",
    "        print(f'Step {step} - Loss {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942d50f-0b11-4665-9ee0-e7ed76bd0c88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
