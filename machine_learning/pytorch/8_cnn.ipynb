{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdda28d4-89ff-46b1-8e04-e0a4f019b98d",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The notebook is intended to experiment with Convolutional Neural Network in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d64a7af1-6c78-4eb4-8ca7-60eaca077446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Standard Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e5bca8-add0-45fc-bcab-b60c3d73442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Seaborn theme parameters\n",
    "theme_parameters =  {\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.top': False,\n",
    "    'grid.alpha':0.3,\n",
    "    'figure.figsize': (16, 6),\n",
    "    'font.family': 'Andale Mono',\n",
    "    'axes.titlesize': 24,\n",
    "    'figure.facecolor': '#E5E8E8',\n",
    "    'axes.facecolor': '#E5E8E8'\n",
    "}\n",
    "\n",
    "# Set the theme\n",
    "sns.set_theme(style='whitegrid',\n",
    "              palette=sns.color_palette('deep'), \n",
    "              rc=theme_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0934eba2-4449-4a73-aec2-a1ae7d4dbf9f",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a40cf337-80a9-4d12-ac4b-341fe7021c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mean and std of the MNIST dataset for the data normalisation\n",
    "# NOTE: We are not going to compute them\n",
    "mean = 0.1307\n",
    "std = 0.3081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b13aef16-73c3-4e50-a816-3d55e2154e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the a set of data transformation through the function \"compose\"\n",
    "data_transformations = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((mean,), (std,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff45eb77-18ad-4e85-94a9-9c1584217f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "train_data = datasets.MNIST(root='./../../data', \n",
    "                            train=True, \n",
    "                            transform=data_transformations, \n",
    "                            download=True)\n",
    "\n",
    "test_data = datasets.MNIST(root='./../../data',\n",
    "                           train=False,\n",
    "                           transform=data_transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2009860d-d641-4b44-b826-18d24b781bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DataLoader for batch training & testing\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, \n",
    "                                                batch_size=100, \n",
    "                                                shuffle=True)\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data, \n",
    "                                               batch_size=100, \n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a7ae0-a6ef-4d3a-85ed-ddeee0a7b6a5",
   "metadata": {},
   "source": [
    "# Plot Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc49ebf6-c775-4033-9f7e-fb41b19422f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve an image and normalise it\n",
    "image = train_data[20][0].numpy() * mean * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086ba894-ab1e-4410-8332-959d7352fb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the image\n",
    "# NOTE: It needs to be reshaped to N x N dimension from a 3-Channel\n",
    "_ = plt.imshow(image.reshape(28, 28),\n",
    "               cmap='gray') # Define image gray color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b0dcf-cbc6-4964-b612-513e594bf3c6",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2734df46-f4fb-4c8d-a013-0226b1889b9a",
   "metadata": {},
   "source": [
    "## Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f78d6c46-af11-4f15-890d-8f332865a173",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialise a Convolutional Neural Network\n",
    "\n",
    "    Attributes:\n",
    "        conv2d_1 nn.Conv2d Convolutional 2-D Layer 1 \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 conv2d_1_attributes: dict, \n",
    "                 batch_normalisation_1_attributes: dict, \n",
    "                 max_pooling_1_attributes: dict, \n",
    "                 conv2d_2_attributes: dict, \n",
    "                 batch_normalisation_2_attributes: dict, \n",
    "                 max_pooling_2_attributes: dict):\n",
    "    \n",
    "        \"\"\"\n",
    "        Initialise the Neural Network\n",
    "\n",
    "        Args:\n",
    "            conv2d_1_attributes Dictionary of conv2d_1 attributes\n",
    "            batch_normalisation_1_attributes Dictionary of batch_normalisation_1 attributes\n",
    "            max_pooling_1_attributes Dictionary of max_pooling_1 attributes\n",
    "            conv2d_2_attributes Dictionary of conv2d_1 attributes\n",
    "            batch_normalisation_2_attributes Dictionary of batch_normalisation_2 attributes\n",
    "            max_pooling_2_attributes Dictionary of max_pooling_2 attributes\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Call the nn.Module constructor\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Define the first Convolutional layer\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels=conv2d_1_attributes['input_channels'], \n",
    "                                  out_channels=conv2d_1_attributes['output_channels'], \n",
    "                                  kernel_size=conv2d_1_attributes['kernel_size'], \n",
    "                                  stride=conv2d_1_attributes['stride'], \n",
    "                                  padding=conv2d_1_attributes['padding'])\n",
    "        \n",
    "        # Define the Batch Normalisation layer for conv2d_1\n",
    "        self.batch_normalisation_1 = nn.BatchNorm2d(batch_normalisation_1_attributes['feature_map_dim'])\n",
    "        \n",
    "        # Specify conv2d_1 activation function\n",
    "        self.activation_function_1 = nn.ReLU()\n",
    "                 \n",
    "        # Max Pooling for conv2d_1\n",
    "        # Note: the final dimension of the feature map after the max_pooling_1 would be\n",
    "        # [(input_size - kernel_size + 2*padding/stride + 1]\n",
    "        # 28 - 3 + 2*1/1 + 1 = 28\n",
    "        # After the MaxPooling: Feature Map Size / Max Pooling Kernel = 28/2 = 14\n",
    "        self.max_pooling_1 = nn.MaxPool2d(max_pooling_1_attributes['kernel_size'])\n",
    "        \n",
    "        # Define the second Convolutional layer\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels=conv2d_2_attributes['input_channels'], \n",
    "                                  out_channels=conv2d_2_attributes['output_channels'], \n",
    "                                  kernel_size=conv2d_2_attributes['kernel_size'], \n",
    "                                  stride=conv2d_2_attributes['stride'], \n",
    "                                  padding=conv2d_2_attributes['padding'])\n",
    "        \n",
    "        # Define the Batch Normalisation layer for conv2d_2\n",
    "        self.batch_normalisation_2 = nn.BatchNorm2d(batch_normalisation_2_attributes['feature_map_dim'])\n",
    "        \n",
    "        # Specify conv2d_2 activation function\n",
    "        self.activation_function_2 = nn.ReLU()\n",
    "        \n",
    "        # Max Pooling for conv2d_2\n",
    "        # Output size: 14 - 5 + 2*2/1 + 1 = 14\n",
    "        # After Max Pooling: 14/2 = 7\n",
    "        self.max_pooling_2 = nn.MaxPool2d(max_pooling_2_attributes['kernel_size'])\n",
    "        \n",
    "        # Define first fully-connected layer\n",
    "        # NOTE: The number of neurons is 32x7x7 = 1568\n",
    "        # NOTE2: 600 is the arbitrary chosen output dimension\n",
    "        self.fully_connected_1 = nn.Linear(1568, 600)\n",
    "        \n",
    "        # Specify fully_connected_1 activation function\n",
    "        self.activation_function_fcc_1 = nn.ReLU()\n",
    "        \n",
    "        # Apply dropout \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Define second fully-connected layer\n",
    "        # NOTE: the output dimension is 10 because of the number of classes\n",
    "        self.fully_connected_2 = nn.Linear(600, 10)\n",
    "                                  \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Implements the feed forward process of the input x through the CNN\n",
    "        \"\"\"\n",
    "                                  \n",
    "        # Feed forward the input through the CNN layers\n",
    "        output_conv2d_1 = self.conv2d_1(x)\n",
    "        output_conv2d_1 = self.batch_normalisation_1(output_conv2d_1)\n",
    "        output_conv2d_1 = self.activation_function_1(output_conv2d_1)\n",
    "        output_conv2d_1 = self.max_pooling_1(output_conv2d_1)\n",
    "        \n",
    "        output_conv2d_2 = self.conv2d_2(output_conv2d_1)\n",
    "        output_conv2d_2 = self.batch_normalisation_2(output_conv2d_2)\n",
    "        output_conv2d_2 = self.activation_function_2(output_conv2d_2)\n",
    "        output_conv2d_2 = self.max_pooling_2(output_conv2d_2)\n",
    "        \n",
    "        # Flatten the output with a batch size passed at runtime (-1)\n",
    "        output_conv2d_2 = output_conv2d_2.view(-1, 1568)\n",
    "        \n",
    "        \n",
    "        output_fcc_1 = self.fully_connected_1(output_conv2d_2)\n",
    "        output_fcc_1 = self.activation_function_fcc_1(output_fcc_1)\n",
    "        output_fcc_1 = self.dropout(output_fcc_1)\n",
    "        \n",
    "        output = self.fully_connected_2(output_fcc_1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aff0e63c-ba3d-4915-9b05-89230f8d7953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "conv2d_1_attributes = {\n",
    "    'input_channels': 1,\n",
    "    'output_channels': 8, # Number of filters to apply\n",
    "    'kernel_size': 3,\n",
    "    'stride': 1,\n",
    "    'padding': 1 # (kernel_size - 1)/2\n",
    "}\n",
    "\n",
    "batch_normalisation_1_attributes = {\n",
    "    'feature_map_dim': 8\n",
    "}\n",
    "\n",
    "max_pooling_1_attributes = {\n",
    "    'kernel_size': 2\n",
    "}\n",
    "\n",
    "conv2d_2_attributes = {\n",
    "    'input_channels': 8,\n",
    "    'output_channels': 32, # Number of filters to apply\n",
    "    'kernel_size': 5,\n",
    "    'stride': 1,\n",
    "    'padding': 2 # (kernel_size - 1)/2\n",
    "}\n",
    "\n",
    "batch_normalisation_2_attributes = {\n",
    "    'feature_map_dim': 32\n",
    "}\n",
    "\n",
    "max_pooling_2_attributes = {\n",
    "    'kernel_size': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824f47dc-fc5c-4afb-91be-1b8530fb7a16",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c8476d1-87cc-447b-b641-b889a70375ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = Model(conv2d_1_attributes, \n",
    "              batch_normalisation_1_attributes, \n",
    "              max_pooling_1_attributes, \n",
    "              conv2d_2_attributes, \n",
    "              batch_normalisation_2_attributes, \n",
    "              max_pooling_2_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c042a7aa-b590-466d-aa2b-5aecaa0872ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "cuda_flag = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcb2936f-8c24-42f4-b8b5-bbd481057e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If CUDA is available, swtich the model to work on the GPU\n",
    "if cuda_flag:\n",
    "    mode = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b35f5f4-9042-4ef2-b80e-76c4ca1be6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1610e109-9bdf-4d2b-a0ab-af2d33fa27c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimiser\n",
    "optimiser = torch.optim.Adam(model.parameters(), \n",
    "                             lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e39669-c2ea-499e-a9c9-87c3f0ee1f95",
   "metadata": {},
   "source": [
    "The input shape would be something like (100, 1, 28, 28), where:\n",
    "- 100 is the batch size\n",
    "- 1 is the number of channels\n",
    "- 28, 28 is the image shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958b985b-c07a-4b18-b809-4041d863c1c0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e71dec01-4492-4e38-947b-12a8bdb794e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training parameters\n",
    "epochs = 10\n",
    "\n",
    "# Initialise the metrics lists\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8e9c790-2b69-4fd6-87c0-c3288066d3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training Loss: 0.703 - Training Accuracy: 79.915 - Testing Loss 0.083 - Testing Accuracy 97.760\n",
      "Epoch 2/10 - Training Loss: 0.173 - Training Accuracy: 94.758 - Testing Loss 0.047 - Testing Accuracy 98.660\n",
      "Epoch 3/10 - Training Loss: 0.099 - Training Accuracy: 97.277 - Testing Loss 0.036 - Testing Accuracy 98.850\n",
      "Epoch 4/10 - Training Loss: 0.080 - Training Accuracy: 97.733 - Testing Loss 0.040 - Testing Accuracy 98.830\n",
      "Epoch 5/10 - Training Loss: 0.068 - Training Accuracy: 98.123 - Testing Loss 0.036 - Testing Accuracy 98.970\n",
      "Epoch 6/10 - Training Loss: 0.063 - Training Accuracy: 98.195 - Testing Loss 0.037 - Testing Accuracy 99.020\n",
      "Epoch 7/10 - Training Loss: 0.060 - Training Accuracy: 98.260 - Testing Loss 0.033 - Testing Accuracy 99.140\n",
      "Epoch 8/10 - Training Loss: 0.059 - Training Accuracy: 98.310 - Testing Loss 0.042 - Testing Accuracy 98.830\n",
      "Epoch 9/10 - Training Loss: 0.053 - Training Accuracy: 98.482 - Testing Loss 0.032 - Testing Accuracy 99.150\n",
      "Epoch 10/10 - Training Loss: 0.053 - Training Accuracy: 98.542 - Testing Loss 0.035 - Testing Accuracy 99.160\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Initialise the epoch's metrics for later evaluation\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Initialise the number of iterations per epoch\n",
    "    iterations = 0\n",
    "    \n",
    "    # Initialise the number of correct predictions\n",
    "    correct_predictions_num = 0\n",
    "    \n",
    "    # Switch the model to the 'train' mode (the other mode is the 'test' one)\n",
    "    # NOTE: Dropout and Batch Normalisation are not required during the testing phase\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    # Fetch the training batches\n",
    "    for index, (inputs, labels) in enumerate(train_data_loader):\n",
    "        \n",
    "        # Transfer the data to CUDA if available\n",
    "        if cuda_flag:    \n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        # Feed forward\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_function(outputs, labels)\n",
    "        \n",
    "        # Accumulate the epoch's loss\n",
    "        # NOTE: the .item() extract the value from the Tensor\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Reset gradient\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimiser.step()\n",
    "        \n",
    "        # Compute the predicted class\n",
    "        # NOTE: torch.max return the value, index pair\n",
    "        _, predicted_class = torch.max(outputs, 1)\n",
    "        \n",
    "        # Compute the correct number of predictions\n",
    "        correct_predictions_num += (predicted_class == labels).sum().item()\n",
    "        \n",
    "        # Increment the iterations\n",
    "        iterations += 1\n",
    "        \n",
    "    # Update training metrics\n",
    "    train_loss.append(epoch_loss/iterations)\n",
    "    train_accuracy.append(100 * correct_predictions_num/len(train_data))\n",
    "    \n",
    "    # Model's Batch Test\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialise epoch's test metrics\n",
    "    epoch_test_loss = 0.0\n",
    "    epoch_test_correct_predictions_num = 0\n",
    "    test_iterations = 0\n",
    "    \n",
    "    # Fetch the testing batches\n",
    "    for index, (inputs, labels) in enumerate(test_data_loader):\n",
    "        \n",
    "        # Transfer the data to CUDA if available\n",
    "        if cuda_flag:    \n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        # Feed Forward\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_function(outputs, labels)\n",
    "        \n",
    "        # Accumulate the epoch's test loss\n",
    "        epoch_test_loss += loss\n",
    "        \n",
    "        # Compute predicted classe\n",
    "        _, predicted_class = torch.max(outputs, 1)\n",
    "        \n",
    "        # Compute the correct number of predictions\n",
    "        epoch_test_correct_predictions_num += (predicted_class == labels).sum().item()\n",
    "        \n",
    "        # Increment the iterations\n",
    "        test_iterations += 1\n",
    "        \n",
    "    # Update test metrics\n",
    "    test_loss.append(epoch_test_loss/test_iterations)\n",
    "    test_accuracy.append(100 * epoch_test_correct_predictions_num/len(test_data))\n",
    "    \n",
    "    print('Epoch {}/{} - Training Loss: {:.3f} - Training Accuracy: {:.3f} - Testing Loss {:.3f} - Testing Accuracy {:.3f}'.format(epoch + 1,\n",
    "                                                                                                                                   epochs,\n",
    "                                                                                                                                   train_loss[-1],\n",
    "                                                                                                                                   train_accuracy[-1],\n",
    "                                                                                                                                   test_loss[-1], \n",
    "                                                                                                                                   test_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacc60b5-00f1-4d75-994d-4dab4fd9d0f1",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a4d7bdd-4ac2-46d0-9630-b41eb732b325",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Construct model_metric DataFrame\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTraining Loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTesting Loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTraining Accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_accuracy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTesting Accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_accuracy\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/cheat_sheets-EiW5VkhA/lib/python3.10/site-packages/pandas/core/frame.py:663\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    657\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    658\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    659\u001b[0m     )\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 663\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/cheat_sheets-EiW5VkhA/lib/python3.10/site-packages/pandas/core/internals/construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    491\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/cheat_sheets-EiW5VkhA/lib/python3.10/site-packages/pandas/core/internals/construction.py:123\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m \u001b[43m_homogenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# _homogenize ensures\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/cheat_sheets-EiW5VkhA/lib/python3.10/site-packages/pandas/core/internals/construction.py:617\u001b[0m, in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    614\u001b[0m             val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(val)\n\u001b[1;32m    615\u001b[0m         val \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mfast_multiget(val, oindex\u001b[38;5;241m.\u001b[39m_values, default\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan)\n\u001b[0;32m--> 617\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_cast_failure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(val, index)\n\u001b[1;32m    622\u001b[0m homogenized\u001b[38;5;241m.\u001b[39mappend(val)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/cheat_sheets-EiW5VkhA/lib/python3.10/site-packages/pandas/core/construction.py:642\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure, allow_2d)\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 642\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_convert_platform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subarr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    644\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, subarr)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/cheat_sheets-EiW5VkhA/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:127\u001b[0m, in \u001b[0;36mmaybe_convert_platform\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    124\u001b[0m arr: ArrayLike\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mrange\u001b[39m)):\n\u001b[0;32m--> 127\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_1d_object_array_from_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# The caller is responsible for ensuring that we have np.ndarray\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m#  or ExtensionArray here.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     arr \u001b[38;5;241m=\u001b[39m values\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/cheat_sheets-EiW5VkhA/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1784\u001b[0m, in \u001b[0;36mconstruct_1d_object_array_from_listlike\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# numpy will try to interpret nested lists as further dimensions, hence\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# making a 1D array that contains list-likes is a bit tricky:\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(values), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1784\u001b[0m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m values\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/cheat_sheets-EiW5VkhA/lib/python3.10/site-packages/torch/_tensor.py:958\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "# Construct model_metric DataFrame\n",
    "model_metrics = pd.DataFrame({'Training Loss': train_loss, \n",
    "                              'Testing Loss': test_loss, \n",
    "                              'Training Accuracy': train_accuracy, \n",
    "                              'Testing Accuracy': test_accuracy}, \n",
    "                             index=np.arange(1, epochs + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8567c590-c839-4fed-bbf3-c8e0df3ae6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training & Test Loss\n",
    "sns.lineplot(\n",
    "    data=, \n",
    "    x='pickup', \n",
    "    y='total', \n",
    "    hue='pickup_borough'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
