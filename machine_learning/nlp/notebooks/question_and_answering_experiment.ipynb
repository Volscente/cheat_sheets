{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The notebook is intened to experiment with different technologies for the task of **Question and Answering**.\n",
    "\n",
    "There are two different **type of models**:\n",
    "- *Open Domain* - They do not require a passed context\n",
    "- *Reading Comprehension* - They find the answer within a given context\n",
    "\n",
    "Such models can work in two different **approaches**:\n",
    "- *Open Book* - The model can access external source of information\n",
    "- *Closed Book* - The model can only access what has been encoded in its paramters\n",
    "\n",
    "The **Components** of an Open Domain Q&A are:\n",
    "- *Retriever* - It finds relevant contexts from an external source given the question (This is the component that differentiate an Open Domain from a Reading Comprehension)\n",
    "- *Reader* - It locates the position in the context where the answer to the question is (alternatively there can be a *Generator*)\n",
    "- *Generator*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQuAD 2.0\n",
    "\n",
    "The SQuAD (Stanford Question and Answering Dataset) is a hugely popular dataset containing question and answer pairs scraped from Wikipedia, covering topics ranging from Beyonce, to Physics. \n",
    "\n",
    "It is possible to retrieve both the Training and Dev set at this [link](https://rajpurkar.github.io/SQuAD-explorer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Standard Libraries\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "with open('./../../../data/squad_train_v2.0.json', 'rb') as file:\n",
    "    squad_train_data = json.load(file)\n",
    "\n",
    "with open('./../../../data/squad_dev_v2.0.json', 'rb') as file:\n",
    "    squad_dev_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qas': [{'question': 'When did Beyonce start becoming popular?',\n",
       "   'id': '56be85543aeaaa14008c9063',\n",
       "   'answers': [{'text': 'in the late 1990s', 'answer_start': 269}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'What areas did Beyonce compete in when she was growing up?',\n",
       "   'id': '56be85543aeaaa14008c9065',\n",
       "   'answers': [{'text': 'singing and dancing', 'answer_start': 207}],\n",
       "   'is_impossible': False},\n",
       "  {'question': \"When did Beyonce leave Destiny's Child and become a solo singer?\",\n",
       "   'id': '56be85543aeaaa14008c9066',\n",
       "   'answers': [{'text': '2003', 'answer_start': 526}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'In what city and state did Beyonce  grow up? ',\n",
       "   'id': '56bf6b0f3aeaaa14008c9601',\n",
       "   'answers': [{'text': 'Houston, Texas', 'answer_start': 166}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'In which decade did Beyonce become famous?',\n",
       "   'id': '56bf6b0f3aeaaa14008c9602',\n",
       "   'answers': [{'text': 'late 1990s', 'answer_start': 276}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'In what R&B group was she the lead singer?',\n",
       "   'id': '56bf6b0f3aeaaa14008c9603',\n",
       "   'answers': [{'text': \"Destiny's Child\", 'answer_start': 320}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'What album made her a worldwide known artist?',\n",
       "   'id': '56bf6b0f3aeaaa14008c9604',\n",
       "   'answers': [{'text': 'Dangerously in Love', 'answer_start': 505}],\n",
       "   'is_impossible': False},\n",
       "  {'question': \"Who managed the Destiny's Child group?\",\n",
       "   'id': '56bf6b0f3aeaaa14008c9605',\n",
       "   'answers': [{'text': 'Mathew Knowles', 'answer_start': 360}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'When did Beyoncé rise to fame?',\n",
       "   'id': '56d43c5f2ccc5a1400d830a9',\n",
       "   'answers': [{'text': 'late 1990s', 'answer_start': 276}],\n",
       "   'is_impossible': False},\n",
       "  {'question': \"What role did Beyoncé have in Destiny's Child?\",\n",
       "   'id': '56d43c5f2ccc5a1400d830aa',\n",
       "   'answers': [{'text': 'lead singer', 'answer_start': 290}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'What was the first album Beyoncé released as a solo artist?',\n",
       "   'id': '56d43c5f2ccc5a1400d830ab',\n",
       "   'answers': [{'text': 'Dangerously in Love', 'answer_start': 505}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'When did Beyoncé release Dangerously in Love?',\n",
       "   'id': '56d43c5f2ccc5a1400d830ac',\n",
       "   'answers': [{'text': '2003', 'answer_start': 526}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'How many Grammy awards did Beyoncé win for her first solo album?',\n",
       "   'id': '56d43c5f2ccc5a1400d830ad',\n",
       "   'answers': [{'text': 'five', 'answer_start': 590}],\n",
       "   'is_impossible': False},\n",
       "  {'question': \"What was Beyoncé's role in Destiny's Child?\",\n",
       "   'id': '56d43ce42ccc5a1400d830b4',\n",
       "   'answers': [{'text': 'lead singer', 'answer_start': 290}],\n",
       "   'is_impossible': False},\n",
       "  {'question': \"What was the name of Beyoncé's first solo album?\",\n",
       "   'id': '56d43ce42ccc5a1400d830b5',\n",
       "   'answers': [{'text': 'Dangerously in Love', 'answer_start': 505}],\n",
       "   'is_impossible': False}],\n",
       " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train_data['data'][0]['paragraphs'][0] # First data are about Beyonce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all the Q&A pairs\n",
    "q_a_train_data = []\n",
    "\n",
    "# Loop through groups -> paragraphs -> qa_pairs\n",
    "for group in squad_train_data['data']:\n",
    "    for paragraph in group['paragraphs']:\n",
    "\n",
    "        # Retrieve context\n",
    "        context = paragraph['context']\n",
    "\n",
    "        for qa_pair in paragraph['qas']:\n",
    "\n",
    "            # Retrieve question\n",
    "            question = qa_pair['question']\n",
    "\n",
    "            # Check if there is 'answers' or 'plausible_answers'\n",
    "            if 'answers' in qa_pair.keys() and len(qa_pair['answers']) > 0:\n",
    "                answer_list = qa_pair['answers']\n",
    "            elif 'plausible_answers' in qa_pair.keys() and len(qa_pair['plausible_answers']) > 0:\n",
    "                answer_list = qa_pair['plausible_answers']\n",
    "            else:\n",
    "                # Check if no answer is given\n",
    "                answer_list = []\n",
    "\n",
    "            # Retrieve just the text from each answer\n",
    "            answer_list = [item['text'] for item in answer_list]\n",
    "\n",
    "            # Remove duplicates\n",
    "            answer_list = list(set(answer_list))\n",
    "\n",
    "            # Add each answer to the dataset\n",
    "            for answer in answer_list:\n",
    "                # append dictionary sample to parsed squad\n",
    "                q_a_train_data.append({\n",
    "                    'question': question,\n",
    "                    'answer': answer,\n",
    "                    'context': context\n",
    "                })\n",
    "\n",
    "# Retrieve all the Q&A pairs\n",
    "q_a_dev_data = []\n",
    "\n",
    "# Loop through groups -> paragraphs -> qa_pairs\n",
    "for group in squad_dev_data['data']:\n",
    "    for paragraph in group['paragraphs']:\n",
    "\n",
    "        # Retrieve context\n",
    "        context = paragraph['context']\n",
    "\n",
    "        for qa_pair in paragraph['qas']:\n",
    "\n",
    "            # Retrieve question\n",
    "            question = qa_pair['question']\n",
    "\n",
    "            # Check if there is 'answers' or 'plausible_answers'\n",
    "            if 'answers' in qa_pair.keys() and len(qa_pair['answers']) > 0:\n",
    "                answer_list = qa_pair['answers']\n",
    "            elif 'plausible_answers' in qa_pair.keys() and len(qa_pair['plausible_answers']) > 0:\n",
    "                answer_list = qa_pair['plausible_answers']\n",
    "            else:\n",
    "                # Check if no answer is given\n",
    "                answer_list = []\n",
    "\n",
    "            # Retrieve just the text from each answer\n",
    "            answer_list = [item['text'] for item in answer_list]\n",
    "\n",
    "            # Remove duplicates\n",
    "            answer_list = list(set(answer_list))\n",
    "\n",
    "            # Add each answer to the dataset\n",
    "            for answer in answer_list:\n",
    "                # append dictionary sample to parsed squad\n",
    "                q_a_dev_data.append({\n",
    "                    'question': question,\n",
    "                    'answer': answer,\n",
    "                    'context': context\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'When did Beyonce start becoming popular?',\n",
       " 'answer': 'in the late 1990s',\n",
       " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_a_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "with open('./../../../data/squad_processed_train.json', 'w') as file:\n",
    "    json.dump(q_a_train_data, file)\n",
    "\n",
    "with open('./../../../data/squad_processed_dev.json', 'w') as file:\n",
    "    json.dump(q_a_dev_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s.porreca/.local/share/virtualenvs/cheat_sheets-EiW5VkhA/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-01-01 23:15:08.778570: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import Standard Libraries\n",
    "import json\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./../../../data/squad_processed_train.json', 'r') as file:\n",
    "    q_a_train_data = json.load(file)\n",
    "\n",
    "with open('./../../../data/squad_processed_dev.json', 'r') as file:\n",
    "    q_a_dev_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the tokenizer and the model\n",
    "tokenizer = BertTokenizer.from_pretrained('deepset/bert-base-cased-squad2')\n",
    "model = BertForQuestionAnswering.from_pretrained('deepset/bert-base-cased-squad2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve few answers\n",
    "answers = []\n",
    "\n",
    "for sample in q_a_train_data[:5]:\n",
    "\n",
    "    answer = qa_pipeline({\n",
    "        'question': sample['question'],\n",
    "        'context': sample['context']\n",
    "    })\n",
    "\n",
    "    answers.append({\n",
    "        'True Label': sample['answer'],\n",
    "        'Prediction': answer['answer'],\n",
    "        'Confidence': answer['score']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'True Label': 'in the late 1990s',\n",
       "  'Prediction': 'late 1990s',\n",
       "  'Confidence': 0.5621357560157776},\n",
       " {'True Label': 'singing and dancing',\n",
       "  'Prediction': 'singing and dancing',\n",
       "  'Confidence': 0.9938411116600037},\n",
       " {'True Label': '2003',\n",
       "  'Prediction': '(2003),',\n",
       "  'Confidence': 0.9965661764144897},\n",
       " {'True Label': 'Houston, Texas',\n",
       "  'Prediction': 'Houston, Texas,',\n",
       "  'Confidence': 0.847782552242279},\n",
       " {'True Label': 'late 1990s',\n",
       "  'Prediction': '1990s',\n",
       "  'Confidence': 0.6779066324234009}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact Match (EM)\n",
    "\n",
    "There are few limitations, since an exact match is quite hard to achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Standard Libraries\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact matches: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the exact matches\n",
    "exact_matches = []\n",
    "\n",
    "for answer in answers:\n",
    "\n",
    "    # Normalise the text\n",
    "    prediction = re.sub('[^0-9a-z ]', '', answer['Prediction'].lower())\n",
    "    true_label = re.sub('[^0-9a-z ]', '', answer['True Label'].lower())\n",
    "\n",
    "    if prediction == true_label:\n",
    "\n",
    "        exact_matches.append(1)\n",
    "\n",
    "    else:\n",
    "\n",
    "        exact_matches.append(0)\n",
    "\n",
    "print(f'Exact matches: {sum(exact_matches)/len(exact_matches)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall Oriented Understudy for Gisting Evaluation (ROUGE)\n",
    "\n",
    "It is a set of metrics and each of them measure how similar a reference text is to the predicted text:\n",
    "- **ROUEE N** - It measures the number of matching N-Grams (group of tokens) between the reference and predicted text\n",
    "    ```python\n",
    "    # Reference text\n",
    "    reference_text = 'The quick brown fox jumps over the lazy dog'\n",
    "\n",
    "    example_1_gram = ['The', 'quick', ...]\n",
    "    example_2_gram = ['The quick', 'quick brown', ...]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa Standard Libraries\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example predicted output and reference\n",
    "model_output = 'hello to the world'\n",
    "reference_output = 'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance the Rogue object\n",
    "rouge_instance = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 1.0, 'p': 0.5, 'f': 0.6666666622222223},\n",
       "  'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "  'rouge-l': {'r': 1.0, 'p': 0.5, 'f': 0.6666666622222223}}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the N-Gram scores\n",
    "rouge_instance.get_scores(model_output, reference_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is Rouge-1 (one-Gram) and Rouge-2 (bi-gram) with F1, Precision and Recall metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve full data for ROUGE\n",
    "model_predictions = [answer['Prediction'] for answer in answers]\n",
    "references = [answer['True Label'] for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 0.5, 'p': 1.0, 'f': 0.6666666622222223},\n",
       "  'rouge-2': {'r': 0.3333333333333333, 'p': 1.0, 'f': 0.4999999962500001},\n",
       "  'rouge-l': {'r': 0.5, 'p': 1.0, 'f': 0.6666666622222223}},\n",
       " {'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995},\n",
       "  'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995},\n",
       "  'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}},\n",
       " {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "  'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "  'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}},\n",
       " {'rouge-1': {'r': 0.5, 'p': 0.5, 'f': 0.4999999950000001},\n",
       "  'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "  'rouge-l': {'r': 0.5, 'p': 0.5, 'f': 0.4999999950000001}},\n",
       " {'rouge-1': {'r': 0.5, 'p': 1.0, 'f': 0.6666666622222223},\n",
       "  'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "  'rouge-l': {'r': 0.5, 'p': 1.0, 'f': 0.6666666622222223}}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_instance.get_scores(model_predictions, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cheat_sheets-EiW5VkhA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
