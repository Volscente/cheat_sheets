{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is intended to experiment with Sentence Similarity techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Standard Libraries\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 399/399 [00:00<00:00, 122kB/s]\n",
      "config.json: 100%|██████████| 625/625 [00:00<00:00, 3.22MB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.02MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.53MB/s]\n",
      "added_tokens.json: 100%|██████████| 2.00/2.00 [00:00<00:00, 5.46kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 498kB/s]\n",
      "pytorch_model.bin: 100%|██████████| 438M/438M [00:05<00:00, 76.4MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model_name = 'sentence-transformers/bert-base-nli-mean-tokens'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text input\n",
    "text = 'hello world what a time to be alive!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentence\n",
    "tokens = tokenizer.encode_plus(text, \n",
    "                               max_length=128, \n",
    "                               truncation=True, \n",
    "                               padding='max_length', \n",
    "                               return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed tokens\n",
    "outputs = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the embeddings of the sentence by taking the last layer output\n",
    "embeddings = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cheat_sheets-EiW5VkhA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
