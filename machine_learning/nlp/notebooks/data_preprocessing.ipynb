{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The notebooks is intended to show NLP Data Preprocessing techniques and how to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Standard Libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "# Download stopwords if not already done\n",
    "# import nltk\n",
    "# nltk.download('stopwords') # Stopwords Removal\n",
    "# nltk.download('wordnet') # Lemmitization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords Removal\n",
    "\n",
    "This process aims to remove stopwords from an input text, in order to reduce the noise introduction for any NLP model.\n",
    "\n",
    "Stopwords are words that are not particularly useful for any NLP model. In the `input_text` you can see some of them: *in*, *not*, *a*, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input text\n",
    "input_text = \"\"\"I’m amazed how often in practice, not only does a @huggingface NLP model solve your problem, but one of their public finetuned checkpoints, is good enough for the job.\n",
    "\n",
    "Both impressed, and a little disappointed how rarely I get to actually train a model that matters :(\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve stopwords list from NLTK\n",
    "stop_words_english = stopwords.words('english')\n",
    "\n",
    "print(stop_words_english[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the stopwords list to a set for speed up process\n",
    "stop_words_english = set(stop_words_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the input text in lowercase and convert it into a list of words\n",
    "input_text = input_text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "input_text_no_stopwords = [word for word in input_text if word not in stop_words_english]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i’m amazed often practice, @huggingface nlp model solve problem, one public finetuned checkpoints, good enough job. impressed, little disappointed rarely get actually train model matters :(\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(input_text_no_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens\n",
    "\n",
    "It is quite common to not just tokenize the text input, but also to add some special tokens like:\n",
    "- [PAD] - Used for maintaining the same length across input sequences\n",
    "- [UNK] - Used when a token is unknown\n",
    "- [CLS] - Used at the start of the input sequence\n",
    "- [SEP] - Used at the end of the input sequence\n",
    "- [MASK] - Used for masking token (e.g., in MLM training)\n",
    "\n",
    "**Example**\n",
    "\n",
    "From: *\"@joebloggs thinks that the NLP models that @huggingface made are super cool\"*\n",
    "\n",
    "To: \"[CLS] [UNK] thinks that the NLP models that [UNK] made are super cool [SEP] [PAD] [PAD]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "It is a Text Normalization technique that involves simplify the text before it's being processed.\n",
    "\n",
    "The final characters of a word are removed, in order to normalize the word to its root version. All the different forms of a word are called *inflections* in linguistics.\n",
    "\n",
    "Keep in mind that this transformation can be performed by sophisticated models and that it is not required by all the models, because it can change the meaning of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to Stemmize\n",
    "words_to_stem = ['happy', 'happiest', 'happier', 'cactus', 'cactii', 'elephant', 'elephants', 'amazed', 'amazing', 'amazingly', 'cement', 'owed', 'maximum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter | Lancaster\n",
      "happi | happy\n",
      "happiest | happiest\n",
      "happier | happy\n",
      "cactu | cact\n",
      "cactii | cacti\n",
      "eleph | eleph\n",
      "eleph | eleph\n",
      "amaz | amaz\n",
      "amaz | amaz\n",
      "amazingli | amaz\n",
      "cement | cem\n",
      "owe | ow\n",
      "maximum | maxim\n"
     ]
    }
   ],
   "source": [
    "# Define two different Stemmers\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "# Create the stemmings of the words in `words_to_stem` with the two different Stemmers\n",
    "stemmed = [(porter.stem(word), lancaster.stem(word)) for word in words_to_stem]\n",
    "\n",
    "print(\"Porter | Lancaster\")\n",
    "for stem in stemmed:\n",
    "    print(f\"{stem[0]} | {stem[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "It is very similar to Stemming, but it casts the words done to a real common word, called \"*Lemma*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input words\n",
    "words = ['amaze', 'amazed', 'amazing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amaze', 'amazed', 'amazing']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nothing happens, because the speech tag is missing (e.g., Verb or Noun)\n",
    "[lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amaze', 'amaze', 'amaze']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize again with speech tag \"VERB\"\n",
    "[lemmatizer.lemmatize(word, wordnet.VERB) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicode Normalization\n",
    "\n",
    "Many particular unicode characters might be a problem when training NLP models. For example, even if two characters looks the same, they might actually have two distinct unicode values. This is called *Canonical Equivalence*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ç Ç\n"
     ]
    }
   ],
   "source": [
    "# Characters look the same\n",
    "print(\"\\u00C7\", \"\\u0043\\u0327\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But of course they are not\n",
    "\"\\u00C7\" == \"\\u0043\\u0327\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important topic is the *Compatibility Equivalence*, where two characters do not look the same, because maybe they are in a different format style, but they are in reality the same letter.\n",
    "\n",
    "<br>\n",
    "\n",
    "The solution for *Canonical Equivalence* is to decompose the characters into single units. While for the *Compatibility Equivalence*, it is necessary to normalize the characters to a common format. This implies the usage of the so called *Normal Forms*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ç'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define an ambiguous character\n",
    "c_with_cedilla = '\\u00C7'\n",
    "c_with_cedilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ç'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the ambiguous character in another way\n",
    "c_plus_cidilla = '\\u0043\\u0327'\n",
    "c_plus_cidilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_with_cedilla == c_plus_cidilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply a Normal Form Decomposition to the Cidilla character\n",
    "unicodedata.normalize('NFD', c_with_cedilla) == c_plus_cidilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is also possible to apply a Normal Form Composition, in order to construct the character from its pieces\n",
    "c_with_cedilla == unicodedata.normalize('NFC', c_plus_cidilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cheat_sheets-EiW5VkhA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
