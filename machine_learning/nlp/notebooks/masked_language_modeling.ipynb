{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is inteded to experiment with Masked Language Modeling task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s.porreca/.local/share/virtualenvs/cheat_sheets-EiW5VkhA/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import Standard Libraries\n",
    "from transformers import BertTokenizer, BertForPreTraining\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 32.6kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 2.06MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 4.71MB/s]\n",
      "config.json: 100%|██████████| 570/570 [00:00<00:00, 1.64MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer for preprocessing\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 440M/440M [00:10<00:00, 42.2MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Initialize the model (NOTE: This is a model version for further fine-tuning BERT)\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input text\n",
    "text = (\"After Abraham Lincoln won the November 1860 presidential [MASK] on an \"\n",
    "        \"anti-slavery platform, an initial seven slave states declared their \"\n",
    "        \"secession from the country to form the Confederacy. War broke out in \"\n",
    "        \"April 1861 when secessionist forces [MASK] Fort Sumter in South \"\n",
    "        \"Carolina, just over a month after Lincoln's inauguration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute tokens and feed them into the model\n",
    "tokens = tokenizer(text, return_tensors='pt')\n",
    "outputs = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['prediction_logits', 'seq_relationship_logits'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **prediction_logits** - It is the output of the MLM head\n",
    "- **seq_relationship_logits** - It is the output of the NSP head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -7.6192,  -7.5433,  -7.6124,  ...,  -6.7155,  -6.7375,  -4.6122],\n",
       "         [-12.5489, -12.3772, -12.6500,  ..., -11.8643, -11.4446,  -9.1151],\n",
       "         [ -6.2346,  -6.3590,  -5.9091,  ...,  -6.1258,  -6.2720,  -5.0268],\n",
       "         ...,\n",
       "         [ -2.2497,  -2.1352,  -2.1812,  ...,  -1.7201,  -1.2728,  -7.8301],\n",
       "         [-14.2654, -14.3100, -14.2294,  ..., -11.4669, -11.7212, -10.3129],\n",
       "         [-11.5071, -12.0389, -11.6046,  ..., -11.2875,  -9.1655,  -9.1733]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.prediction_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 62, 30522])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.prediction_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value `62` is the number of tokens that has been created from the initial text, while `30522` is the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.8256, -1.6897]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.seq_relationship_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the fact that our input is not set to work for NSP, the meaning of the `seq_relationship_logits` does not make much sense.\n",
    "\n",
    "<br>\n",
    "\n",
    "Now let's retrieve the predicted tokens from MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mapping from token IDs to the Vocabulary Index (Token-ID)\n",
    "token2idx = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7592"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show mapping example\n",
    "token2idx['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an inverted dictionary ID-Token\n",
    "idx2token = {value:key for key, value in token2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show mapping example\n",
    "idx2token[7592]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30522])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's map the second token of the outputs\n",
    "outputs.prediction_logits[0][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits into probabilities\n",
    "softmax = torch.nn.functional.softmax(outputs.prediction_logits[0][2], dim=-1)\n",
    "\n",
    "# Retrieve the max probability\n",
    "argmax = torch.argmax(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abraham'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find predicted token\n",
    "idx2token[argmax.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##ecin although abraham lincolnshire won 1948 november 1860 presidential primaries on his anti - slavery platform , an initial seven tributary states declare their independence from the country to form ##ici confederacy ##yre war broke out in april 1861 when ##oya ##ist forces occupied fort sum ##mer for south carolina ##trip just over a month before grant ' s inauguration ; ##tson "
     ]
    }
   ],
   "source": [
    "# Retrieve all predicted tokens\n",
    "# Convert logits into probabilities\n",
    "softmax = torch.nn.functional.softmax(outputs.prediction_logits[0], dim=0)\n",
    "\n",
    "# Retrieve the max probability\n",
    "argmax = torch.argmax(softmax, dim=1)\n",
    "\n",
    "for index in argmax:\n",
    "    print(idx2token[index.item()], end=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cheat_sheets-EiW5VkhA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
